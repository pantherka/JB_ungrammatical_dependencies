{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блоки для тестового задания\n",
    "\n",
    "В jupyter-notebook, чтобы было видно результаты численных проверок градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нужно реализовать: \n",
    "- Embedding (полносвязный линейный слой)\n",
    "- Sigmoid +\n",
    "- ReLU +\n",
    "- Linear +\n",
    "- Softmax +\n",
    "- Tanh +\n",
    "- check_gradient + (сравнение с численным градиентом)\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Численный градиент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(func, X, gradient):\n",
    "    '''\n",
    "    Computes numerical gradient and compares it with analytcal.\n",
    "    func: callable, function of which gradient we are interested. Example call: func(X)\n",
    "    X: np.array of size (n x m)\n",
    "    gradient: np.array of size (n x m)\n",
    "    Returns: maximum absolute diviation between numerical gradient and analytical.\n",
    "    '''\n",
    "    eps = 10**(-5)\n",
    "    \n",
    "    f_grad = np.zeros(gradient.shape)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            X[i, j] += eps            \n",
    "            f_r = func(X)\n",
    "            X[i, j] -= 2 * eps  \n",
    "            f_l = func(X)\n",
    "            X[i, j] += eps\n",
    "                    \n",
    "            f_grad[i, j] = (f_r - f_l) / (2 * eps)            \n",
    "    \n",
    "    return np.max(np.abs(gradient - f_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.574186336839034e-10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверим корректность работы проверки градиента\n",
    "np.random.seed(159)\n",
    "x = np.random.rand(10, 5)\n",
    "func = lambda x: np.sum(x ** 2)\n",
    "gradient = 2 * x\n",
    "check_gradient(func, x, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Собираем блоки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Creates weights and biases for linear layer.\n",
    "        Dimention of inputs is *input_size*, of output: *output_size*.\n",
    "        '''\n",
    "        self.W = np.random.randn(input_size, output_size)*0.01\n",
    "        self.b = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, input_size).\n",
    "        Returns output of size (N, output_size).\n",
    "        '''\n",
    "        self.X = X\n",
    "        return X.dot(self.W)+self.b\n",
    "\n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdw and dLdx.\n",
    "        2. Store dLdw for step() call\n",
    "        3. Return dLdx\n",
    "        '''\n",
    "        self.dLdW = self.X.T.dot(dLdy)\n",
    "        self.dLdb = dLdy.sum(0)\n",
    "        self.dLdx = dLdy.dot(self.W.T)\n",
    "        return self.dLdx\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        '''\n",
    "        1. Apply gradient dLdw to network:\n",
    "        w <- w - learning_rate*dLdw\n",
    "        '''\n",
    "        self.W = self.W - learning_rate * self.dLdW\n",
    "        self.b = self.b - learning_rate * self.dLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.583001059354501e-12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(777)\n",
    "\n",
    "X = np.array([[0, 1], [1, 1]], dtype=float)\n",
    "Y = np.array([[0], [1]])\n",
    "\n",
    "l = Linear(2, 1)\n",
    "\n",
    "def loss(W):\n",
    "    l.W = W\n",
    "    res = l.forward(X)\n",
    "    return np.sum((res - Y)**2)\n",
    "\n",
    "res = l.forward(X)\n",
    "dLdy = 2*(res - Y)\n",
    "\n",
    "l.backward(dLdy)\n",
    "\n",
    "check_gradient(loss, l.W, l.dLdW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нелинейности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, d)\n",
    "        '''\n",
    "        self.sigm = 1.0 / (1 + np.exp(-X))\n",
    "        \n",
    "        return self.sigm\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdx.\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        \n",
    "        self.dLdx = dLdy * self.sigm * (1 - self.sigm)\n",
    "        return self.dLdx\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.783617946202412e-12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(777)\n",
    "\n",
    "X = np.array([[0, 1], [1, 1]], dtype=float)\n",
    "Y = np.array([[0], [1]])\n",
    "\n",
    "l = Sigmoid()\n",
    "\n",
    "def loss(X):\n",
    "    res = l.forward(X)\n",
    "    \n",
    "    return np.sum((res - Y)**2)\n",
    "\n",
    "res = l.forward(X)\n",
    "dLdy = 2*(res - Y)\n",
    "\n",
    "l.backward(dLdy)\n",
    "\n",
    "check_gradient(loss, X, l.dLdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, d)\n",
    "        '''\n",
    "        self.mask = np.zeros(X.shape)\n",
    "        self.mask[X > 0] = 1\n",
    "        \n",
    "        return self.mask * X\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdx.\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        self.dLdx = dLdy * self.mask\n",
    "        return self.dLdx\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.000000413701855e-06"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(777)\n",
    "\n",
    "X = np.array([[0, 1], [1, 1]], dtype=float)\n",
    "Y = np.array([[0], [1]])\n",
    "\n",
    "l = ReLU()\n",
    "\n",
    "def loss(X):\n",
    "    res = l.forward(X)\n",
    "    \n",
    "    return np.sum((res - Y)**2)\n",
    "\n",
    "res = l.forward(X)\n",
    "dLdy = 2*(res - Y)\n",
    "\n",
    "l.backward(dLdy)\n",
    "\n",
    "check_gradient(loss, X, l.dLdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "        max_prob = np.max(X, axis=1, keepdims=True)\n",
    "        p_exp = np.exp(self.X - max_prob)\n",
    "        loss = self.X - np.log(np.sum(p_exp, axis=1, keepdims=True)) - max_prob\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        prob = np.exp(self.X)\n",
    "        prob /= np.sum(prob, axis=1, keepdims=True)\n",
    "        self.dLdx = dLdy * (1 - prob)\n",
    "        return self.dLdx\n",
    "        \n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.55144471 -0.55144471 -1.55144471]\n",
      " [-1.09861229 -1.09861229 -1.09861229]]\n",
      "[[-0.5         0.67957046  0.13447071]\n",
      " [ 0.25       -0.5         0.25      ]]\n",
      "[[-0.39402922  0.28805844  0.10597078]\n",
      " [ 0.16666667 -0.33333333  0.16666667]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0144218798302518e-11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(777)\n",
    "\n",
    "X = np.array([[0, 1, 0], [1, 1, 1]], dtype=float)\n",
    "Y = np.array([[0], [1]])\n",
    "\n",
    "l = LogSoftmax()\n",
    "\n",
    "mask = np.zeros(X.shape)\n",
    "mask[np.arange(X.shape[0]), Y.flatten()] = 1\n",
    "prob = np.exp(X)\n",
    "prob /= np.sum(prob, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def loss(X):\n",
    "    res = l.forward(X)\n",
    "    \n",
    "    curr_loss = -np.mean(np.sum(res * mask, axis=1), axis=0)\n",
    "    \n",
    "    return curr_loss\n",
    "\n",
    "res = l.forward(X)\n",
    "print(res)\n",
    "dLdy = (- 1 + (1 - mask) / (1 - prob)) / X.shape[0]\n",
    "print(dLdy)\n",
    "\n",
    "l.backward(dLdy)\n",
    "print(l.dLdx)\n",
    "\n",
    "check_gradient(loss, X, l.dLdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    '''\n",
    "    Tanh(x) = 2 * Sigmoid(2x) - 1\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, a=1):\n",
    "        self.a = a\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, d)\n",
    "        '''\n",
    "        self.tanh = 2 / (1 + np.exp(-2 * X)) - 1\n",
    "        return self.tanh\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdx.\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        self.dLdx = dLdy * (1 - self.tanh ** 2)\n",
    "        return self.dLdx\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9894808850627896e-11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(777)\n",
    "\n",
    "X = np.array([[0, 1], [1, 1]], dtype=float)\n",
    "Y = np.array([[0], [1]])\n",
    "\n",
    "l = Tanh()\n",
    "\n",
    "def loss(X):\n",
    "    res = l.forward(X)\n",
    "    \n",
    "    return np.sum((res - Y)**2)\n",
    "\n",
    "res = l.forward(X)\n",
    "dLdy = 2*(res - Y)\n",
    "\n",
    "l.backward(dLdy)\n",
    "\n",
    "check_gradient(loss, X, l.dLdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Реализация не закончена, есть только прямой проход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gate_size = 4 * self.hidden_size\n",
    "        \n",
    "        self.W_i = np.zeros((self.input_size, self.gate_size))\n",
    "        \n",
    "        self.W_h = np.zeros((self.hidden_size, self.gate_size))\n",
    "        \n",
    "        self.b = np.zeros(self.gate_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        \n",
    "        h, c = hidden\n",
    "        bs = h.size(0)\n",
    "        \n",
    "        seq_length = inputs.size(0)\n",
    "        out = np.array(seq_length, bs, self.hidden_size)\n",
    "        \n",
    "        for i, inp in enumerate(inputs):\n",
    "            S = (np.matmul(inp, self.W_i) + np.matmul(h, self.W_h) + self.b)\n",
    "            S = S.view(bs, 4, self.hidden_size)\n",
    "            \n",
    "            i_1 = Sigmoid(S[:,0,:])\n",
    "            f_1 = Sigmoid(S[:,1,:])\n",
    "            g_1 = Tanh(S[:,2,:])\n",
    "            o_1 = Sigmoid(S[:,3,:])\n",
    "            \n",
    "            c = f_1 * c + i_1 * g_1\n",
    "            h = o_1 * Tanh(c)\n",
    "            out[i] = h\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
